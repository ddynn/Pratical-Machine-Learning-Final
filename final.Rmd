---
title: "Practical Machine Learning"
author: "ydong"
date: "May 20, 2019"
output: html_document
---

#Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

&nbsp;

The goal of this project is to predict the manner in which people did the exercise. Include the model building, cross validation and use the model to predict 20 more different test cases.


```{r,message=F,warning=F}
library(caret)
library(rattle)
library(caret)
library(rpart)
library(randomForest)
library(gbm)
```

#Load data 
```{r,message=F,warning=F}
train <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header = T)
test<- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header = T)
dim(train);dim(test)
```

The training dataset includes 160 variables, 19622 observation. Here we ignore the varialbes with missing values for now.

#Data cleaning
```{r,message=F,warning=F}
#remove variables that contains missing values
#remove the first seven varialbes
training <- train[,colSums(is.na(train))==0]
training <- training[,-c(1:7)]
dim(training)


# Repeat for the test set
testing <- test[,colSums(is.na(test))==0]
testing <- testing[,-c(1:7)]
dim(testing)

```

Preparing the datasets for prediction
Preparing the data for prediction by splitting the training data into 70% as train data and 30% as test data. This splitting helps to compute the out-of-sample errors.

```{r,message=F,warning=F}
#Data slicing to training and testing dataset 
set.seed(7777777)
intrain <- createDataPartition(training$classe, p=0.7, list=F)
train1 <- training[intrain,]
test1 <- training[-intrain,]
dim(train1);dim(test1)


#Cleaning further by removing the variables that are near-zero-variance
nzv <- nearZeroVar(training)
train1 <- train1[, -nzv]
test1  <- test1[, -nzv]
dim(train1);dim(test1)

```

Here we use the findCorrelation function to search for highly correlated attributes with a cut off of 0.8

```{r}
cor_matrix <- cor(train1[, -53]) 
#remove the y variable for correlation matirx
hcr = findCorrelation(cor_matrix, cutoff=0.8)
names(train1)[hcr]
```

#Model building
For this project,classification trees and random forests and boosing are applied to predict the outcome.

###Train with classification tree

```{r,message=F,warning = F}
library(rpart)
trcontrol<- trainControl(method="cv", number=3,verboseIter = F)
model.ct <- train(classe~., data=train1, method="rpart", trControl=trcontrol)

fancyRpartPlot(model.ct$finalModel)

```

```{r}
pred.ct <- predict(model.ct,newdata=test1)
confm.ct<- confusionMatrix(test1$classe,pred.ct)

# display confusion matrix and model accuracy
confm.ct$table;confm.ct$overall[1]
```
The accuracy is below 0.5, the out-of sample error is about 0.5, suggesting the model is not good enough.


###Train with random forests
```{r}
model.rf <- train(classe~., data=train1, method="rf", trControl=trcontrol)
print(model.rf)
plot(model.rf,main="Accuracy of Random forest model by number of predictors")

pred.rf <- predict(model.rf,newdata=test1)

confmrf <- confusionMatrix(test1$classe,pred.rf)

# display confusion matrix and model accuracy
confmrf$table;confmrf$overall[1]
```
The accuracy rate using the random forest is 0.993,the out-of-sample error is about 0.007,it might be overfitting.

```{r}
model.rf$finalModel$classes
plot(model.rf$finalModel,main="Model error of Random forest by the number of trees",xlim=c(0,100))
# Compute the variable importance 
mostimpvar<- varImp(model.rf)
mostimpvar
#only show the 20 most important variables 
```

###Train with boosting method

```{r,warning= F}
model.bt<- train(classe~., data=train1, method="gbm", trControl=trcontrol, verbose=F)
print(model.bt)
plot(model.bt)
pred.bt <- predict(model.bt,newdata=test1)

confm.bt <- confusionMatrix(test1$classe,pred.bt)

confm.bt$table;confm.bt$overall[1]

```
The accuracy is 0.959, therefore the out-of-sample error is 0.041.

```{r}
finalmodel <- predict(model.bt,newdata=testing)
finalmodel
```